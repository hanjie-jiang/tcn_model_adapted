{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50e33d78",
   "metadata": {},
   "source": [
    "### adapted from locuslab/TCN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe58b28",
   "metadata": {},
   "source": [
    "#### importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8530c02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import csv\n",
    "import re\n",
    "from matplotlib import cm\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_validate\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30da8bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9cf9125",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from filelock import FileLock\n",
    "from torch.utils.data import random_split\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cd9c88",
   "metadata": {},
   "source": [
    "#### data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15bfae2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(path, pattern_interest): \n",
    "    # reading in the data\n",
    "    data = pd.read_csv(path,header=0)\n",
    "    # sorting data in dataframe\n",
    "    df1 = pd.DataFrame(data)\n",
    "    \n",
    "    #getting only the step 11 related variables\n",
    "    step11vars = [col for col in df1.columns if col.split('.')[0] == '11']\n",
    "    df2 = df1.loc[:, df1.columns.isin(step11vars)]\n",
    "    \n",
    "    # getting only the faceplate, bottom heater and heater outerzone related variables within the step11 dataset\n",
    "\n",
    "    # bottom heater related\n",
    "    bottom_related = [col for col in df2.columns if 'Bottom' in col]\n",
    "    dfb = df2.loc[:, df2.columns.isin(bottom_related)]\n",
    "\n",
    "    # faceplate related\n",
    "    faceplate_related = [col for col in df2.columns if \"Faceplate\" in col]\n",
    "    dff = df2.loc[:, df2.columns.isin(faceplate_related)]\n",
    "\n",
    "    # heater outer-zone related\n",
    "    outer_related = [col for col in df2.columns if \"HeaterOuter\" in col]\n",
    "    dfo = df2.loc[:, df2.columns.isin(outer_related)]\n",
    "    \n",
    "    # heater exchanger related\n",
    "    hx_related = [col for col in df2.columns if \"HX\" in col]\n",
    "    dfx = df2.loc[:, df2.columns.isin(hx_related)]\n",
    "    \n",
    "    # heater exchanger related\n",
    "    inner_related = [col for col in df2.columns if \"HeaterInner\" in col]\n",
    "    dfi = df2.loc[:, df2.columns.isin(inner_related)]\n",
    "    \n",
    "    # merge the three keywords related datasets together into `dfall`\n",
    "    dfall = pd.concat([dfb, dff, dfo, dfx, dfi], axis=1)\n",
    "    \n",
    "    mean_related = [col for col in dfall.columns if \"mean\" in col]\n",
    "    dfmean = dfall.loc[:, dfall.columns.isin(mean_related)]\n",
    "    \n",
    "    #Xbotmean = dfmean[dfmean.columns.drop(list(dfmean.filter(regex='11.FaceplateHeater_Temperature.mean')))]\n",
    "    Xmean = dfmean[dfmean.columns.drop(list(dfmean.filter(regex=pattern_interest)))]\n",
    "    ymean = dfmean.loc[:,[pattern_interest]]\n",
    "    \n",
    "    return Xmean, ymean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6af038f",
   "metadata": {},
   "source": [
    "#### TCN module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8719558e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import weight_norm\n",
    "\n",
    "class Chomp1d(nn.Module):\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size].contiguous()\n",
    "\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
    "                                 self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv2.weight.data.normal_(0, 0.01)\n",
    "        if self.downsample is not None:\n",
    "            self.downsample.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)\n",
    "\n",
    "\n",
    "class TemporalConvNet(nn.Module):\n",
    "    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n",
    "        super(TemporalConvNet, self).__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
    "                                     padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0719b979",
   "metadata": {},
   "source": [
    "#### upper level of the TCN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c579251",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TCN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_channels, kernel_size, dropout):\n",
    "        super(TCN, self).__init__()\n",
    "        self.tcn = TemporalConvNet(input_size, num_channels, kernel_size=kernel_size, dropout=dropout)\n",
    "        self.linear = nn.Linear(num_channels[-1], output_size)\n",
    "        self.init_weights() # if the data needs to be weighted\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.linear.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y1 = self.tcn(x)  # if multidimensional, input should have dimension (N, C, L)\n",
    "        o = self.linear(y1[:, :, -1])\n",
    "        return o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a91209",
   "metadata": {},
   "source": [
    "#### data generator for the adding problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45409e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "def data_generator(percent_train, seq_length, path):\n",
    "    \n",
    "    \"\"\"\n",
    "    Args:\n",
    "        seq_length: Length of the adding problem data\n",
    "        starting_index: the index of where the sequence starts; starting from 0\n",
    "    \"\"\"\n",
    "    # Specifying the feature columns that we need for predictions\n",
    "    col1 = '11.BottomHeater_Temperature.mean'\n",
    "    col2 = '11.FaceplateHeater_Temperature.mean'\n",
    "    col3 = '11.HeaterOuterZone_Temperature.mean'\n",
    "    col = [col1, col2, col3]\n",
    "    \n",
    "    # Giving the path to the data set\n",
    "    csv = path\n",
    "    \n",
    "    # Data cleaning, getting the feature matrix and \n",
    "    X_bot, y_bot = data_cleaning(csv, col3)\n",
    "    \n",
    "    X_bot = torch.tensor(X_bot.to_numpy()).float()\n",
    "    y_bot = torch.tensor(y_bot.to_numpy()).float()\n",
    "    print(\"converted X_bot \", X_bot.shape, \"and y_bot \", y_bot.shape, \" to tensors\\n\")\n",
    "    \n",
    "    X_bot1 = torch.reshape(X_bot, [1, X_bot.shape[1], X_bot.shape[0]]) #[1,14,274]\n",
    "    y_bot1 = torch.reshape(y_bot, [1, y_bot.shape[0]]) #[1,274]\n",
    "\n",
    "    first_test_idx = int(round(percent_train * X_bot.shape[0]))\n",
    "    X_train = X_bot1[:, :, 0:first_test_idx]\n",
    "    X_test = X_bot1[:, :, first_test_idx:]\n",
    "    y_train = y_bot1[:, 0:first_test_idx]\n",
    "    y_test = y_bot1[:, first_test_idx:]\n",
    "    \n",
    "    X_train_final = torch.zeros([X_train.shape[2]-seq_length, X_train.shape[1], seq_length]) #[196, 14, 10] = [206-10, 14, 10]\n",
    "    y_train_final = torch.zeros([y_train.shape[1]-seq_length, 1]) #[196, 1] = [206-10, 1]\n",
    "    X_test_final = torch.zeros([X_test.shape[2]-seq_length, X_test.shape[1], seq_length]) #[58, 14, 10] = [68-10, 14, 10]\n",
    "    y_test_final = torch.zeros([y_test.shape[1]-seq_length, 1]) #[58,1] = [68-10, 1]\n",
    "    \n",
    "    for i in range(X_train.shape[2]-seq_length):\n",
    "        X_train_final[i, :, :] = X_train[: , : , i:i+seq_length]\n",
    "        y_train_final[i, :] = y_train[: , i+seq_length:i+seq_length+1]\n",
    "        \n",
    "    for i in range(X_test.shape[2]-seq_length):\n",
    "        X_test_final[i, :, :] = X_test[: , : , i:i+seq_length]\n",
    "        y_test_final[i, :] = y_test[: , i+seq_length:i+seq_length+1]\n",
    "#         print(\"at index i = \", i, \"y_test[1, \", i+seq_length, \"] = \",y_test[: , i+seq_length:i+seq_length+1])\n",
    "#         print(\"at index i = \", i, \"y_test_final[\",i, \", :] = \", y_test_final[i, :])\n",
    "\n",
    "    \n",
    "#     print(\"Y_train shape is set to: \", y_train_final.shape, \"; Y_train type is: \", type(y_train_final), \"\\n\")\n",
    "#     print(\"X_train shape is set to: \", X_train_final.shape, \"; X_train type is: \", type(X_train_final), \"\\n\")\n",
    "#     print(\"Y_test shape is set to: \", y_test_final.shape, \"; Y_test type is: \", type(y_test_final), \"\\n\")\n",
    "#     print(\"X_test shape is set to: \", X_test_final.shape, \"; X_test type is: \", type(X_test_final), \"\\n\")    \n",
    "    \n",
    "    return Variable(X_train_final), Variable(y_train_final), Variable(X_test_final), Variable(y_test_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d987e44",
   "metadata": {},
   "source": [
    "#### parameters needed for the program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "715f4970",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_channels = 14 # # of input features\n",
    "n_classes = 1  #size of each output sample; output feature sizes of y_pred\n",
    "batch_size = 16 #batch size defaults to 32\n",
    "seq_length = 5 #sequence length; defaults to 400\n",
    "epochs = 25 #upper epoch limit; defaults to 10\n",
    "clip = -1 #args.clip; gradient clip, -1 means no clip (default: -1)\n",
    "log_interval = 5 #args.log_interval; 'report interval (default: 100')\n",
    "channel_sizes = [70]*7 #[args.nhid]*args.levels; number of hidden units per layer * # of levels\n",
    "kernel_size = 5 #args.ksize; kernel_size\n",
    "dropout =  0.0 #args.dropout; dropouts applied to each layer; defaults to 0.0\n",
    "model = TCN(input_channels, n_classes, channel_sizes, kernel_size=kernel_size, dropout=dropout)\n",
    "lr = 0.0027257365073673117 # learning rate is defaulted to be 0.0040\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr) #optimizer = getattr(optim, args.optim)(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efa65da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converted X_bot  torch.Size([274, 14]) and y_bot  torch.Size([274, 1])  to tensors\n",
      "\n",
      "torch.Size([214, 14, 5]) torch.Size([214, 1]) torch.Size([50, 14, 5]) torch.Size([50, 1])\n"
     ]
    }
   ],
   "source": [
    "percent = 0.8\n",
    "X_train, Y_train, X_test, Y_test = data_generator(percent, seq_length, r\"C:\\Users\\e177321\\Documents\\data\\Marathon_data.csv\")\n",
    "print(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e784c8c3",
   "metadata": {},
   "source": [
    "#### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce8873bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, input_channels=14):\n",
    "    global lr\n",
    "    \n",
    "    model.train()\n",
    "    batch_idx = 1\n",
    "    total_loss = 0\n",
    "    for i in range(0, X_train.size(0), batch_size):\n",
    "        if i + batch_size > X_train.size(0):\n",
    "            x, y = X_train[i:], Y_train[i:]\n",
    "        else:\n",
    "            x, y = X_train[i:(i+batch_size)], Y_train[i:(i+batch_size)]\n",
    "            \n",
    "        #print(\"x.shape = \", x.shape, \"y.shape = \", y.shape)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        loss = torch.nn.functional.mse_loss(output, y)\n",
    "        loss.backward()\n",
    "        if clip > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        batch_idx += 1\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            processed = min(i+batch_size, X_train.size(0))\n",
    "            print('Train Epoch: {:2d} [{:6d}/{:6d} ({:.0f}%)]\\tLearning rate: {:.4f}\\tLoss: {:.6f}'.format(\n",
    "                epoch, processed, X_train.size(0), 100.*processed/X_train.size(0), lr, cur_loss))\n",
    "            total_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0e5073",
   "metadata": {},
   "source": [
    "#### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55ae7c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(X_test)\n",
    "        test_loss = torch.nn.functional.mse_loss(output, Y_test)\n",
    "        print('\\nTest set: Average loss: {:.6f}\\n'.format(test_loss.item()))\n",
    "        return Y_test, output, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f24f572d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:  1 [    64/   214 (30%)]\tLearning rate: 0.0027\tLoss: 225850.382812\n",
      "Train Epoch:  1 [   144/   214 (67%)]\tLearning rate: 0.0027\tLoss: 19206.827734\n",
      "Train Epoch:  1 [   214/   214 (100%)]\tLearning rate: 0.0027\tLoss: 5770.830688\n",
      "\n",
      "Test set: Average loss: 8102.728516\n",
      "\n",
      "Train Epoch:  2 [    64/   214 (30%)]\tLearning rate: 0.0027\tLoss: 3876.397644\n",
      "Train Epoch:  2 [   144/   214 (67%)]\tLearning rate: 0.0027\tLoss: 2905.061963\n",
      "Train Epoch:  2 [   214/   214 (100%)]\tLearning rate: 0.0027\tLoss: 2097.494885\n",
      "\n",
      "Test set: Average loss: 1565.086548\n",
      "\n",
      "Train Epoch:  3 [    64/   214 (30%)]\tLearning rate: 0.0027\tLoss: 719.056229\n",
      "Train Epoch:  3 [   144/   214 (67%)]\tLearning rate: 0.0027\tLoss: 692.496271\n",
      "Train Epoch:  3 [   214/   214 (100%)]\tLearning rate: 0.0027\tLoss: 366.852910\n",
      "\n",
      "Test set: Average loss: 71.766228\n",
      "\n",
      "Train Epoch:  4 [    64/   214 (30%)]\tLearning rate: 0.0027\tLoss: 179.325499\n",
      "Train Epoch:  4 [   144/   214 (67%)]\tLearning rate: 0.0027\tLoss: 131.810144\n",
      "Train Epoch:  4 [   214/   214 (100%)]\tLearning rate: 0.0027\tLoss: 73.776312\n",
      "\n",
      "Test set: Average loss: 79.320930\n",
      "\n",
      "Train Epoch:  5 [    64/   214 (30%)]\tLearning rate: 0.0027\tLoss: 50.432690\n",
      "Train Epoch:  5 [   144/   214 (67%)]\tLearning rate: 0.0027\tLoss: 42.757163\n",
      "Train Epoch:  5 [   214/   214 (100%)]\tLearning rate: 0.0027\tLoss: 30.578402\n",
      "\n",
      "Test set: Average loss: 10.192430\n",
      "\n",
      "Train Epoch:  6 [    64/   214 (30%)]\tLearning rate: 0.0027\tLoss: 11.907610\n",
      "Train Epoch:  6 [   144/   214 (67%)]\tLearning rate: 0.0027\tLoss: 8.918767\n",
      "Train Epoch:  6 [   214/   214 (100%)]\tLearning rate: 0.0027\tLoss: 6.476638\n",
      "\n",
      "Test set: Average loss: 10.377707\n",
      "\n",
      "Train Epoch:  7 [    64/   214 (30%)]\tLearning rate: 0.0027\tLoss: 3.734035\n",
      "Train Epoch:  7 [   144/   214 (67%)]\tLearning rate: 0.0027\tLoss: 3.165154\n",
      "Train Epoch:  7 [   214/   214 (100%)]\tLearning rate: 0.0027\tLoss: 1.407166\n",
      "\n",
      "Test set: Average loss: 2.655074\n",
      "\n",
      "Train Epoch:  8 [    64/   214 (30%)]\tLearning rate: 0.0027\tLoss: 1.269464\n",
      "Train Epoch:  8 [   144/   214 (67%)]\tLearning rate: 0.0027\tLoss: 1.132589\n",
      "Train Epoch:  8 [   214/   214 (100%)]\tLearning rate: 0.0027\tLoss: 0.399412\n",
      "\n",
      "Test set: Average loss: 0.630559\n",
      "\n",
      "Train Epoch:  9 [    64/   214 (30%)]\tLearning rate: 0.0027\tLoss: 0.294545\n",
      "Train Epoch:  9 [   144/   214 (67%)]\tLearning rate: 0.0027\tLoss: 0.228940\n",
      "Train Epoch:  9 [   214/   214 (100%)]\tLearning rate: 0.0027\tLoss: 0.093503\n",
      "\n",
      "Test set: Average loss: 0.048486\n",
      "\n",
      "Train Epoch: 10 [    64/   214 (30%)]\tLearning rate: 0.0027\tLoss: 0.036895\n",
      "Train Epoch: 10 [   144/   214 (67%)]\tLearning rate: 0.0027\tLoss: 0.029126\n",
      "Train Epoch: 10 [   214/   214 (100%)]\tLearning rate: 0.0027\tLoss: 0.038087\n",
      "\n",
      "Test set: Average loss: 0.107814\n",
      "\n",
      "Train Epoch: 11 [    64/   214 (30%)]\tLearning rate: 0.0027\tLoss: 0.137021\n",
      "Train Epoch: 11 [   144/   214 (67%)]\tLearning rate: 0.0027\tLoss: 0.073662\n",
      "Train Epoch: 11 [   214/   214 (100%)]\tLearning rate: 0.0027\tLoss: 0.052941\n",
      "\n",
      "Test set: Average loss: 0.160264\n",
      "\n",
      "Train Epoch: 12 [    64/   214 (30%)]\tLearning rate: 0.0027\tLoss: 0.168853\n",
      "Train Epoch: 12 [   144/   214 (67%)]\tLearning rate: 0.0027\tLoss: 0.055485\n",
      "Train Epoch: 12 [   214/   214 (100%)]\tLearning rate: 0.0027\tLoss: 0.040374\n",
      "\n",
      "Test set: Average loss: 0.072862\n",
      "\n",
      "Train Epoch: 13 [    64/   214 (30%)]\tLearning rate: 0.0027\tLoss: 0.115965\n",
      "Train Epoch: 13 [   144/   214 (67%)]\tLearning rate: 0.0027\tLoss: 0.034272\n",
      "Train Epoch: 13 [   214/   214 (100%)]\tLearning rate: 0.0027\tLoss: 0.029993\n",
      "\n",
      "Test set: Average loss: 0.049527\n",
      "\n",
      "Train Epoch: 14 [    64/   214 (30%)]\tLearning rate: 0.0027\tLoss: 0.094879\n",
      "Train Epoch: 14 [   144/   214 (67%)]\tLearning rate: 0.0027\tLoss: 0.026688\n",
      "Train Epoch: 14 [   214/   214 (100%)]\tLearning rate: 0.0027\tLoss: 0.025293\n",
      "\n",
      "Test set: Average loss: 0.041276\n",
      "\n",
      "Train Epoch: 15 [    64/   214 (30%)]\tLearning rate: 0.0027\tLoss: 0.092086\n",
      "Train Epoch: 15 [   144/   214 (67%)]\tLearning rate: 0.0027\tLoss: 0.024678\n",
      "Train Epoch: 15 [   214/   214 (100%)]\tLearning rate: 0.0027\tLoss: 0.022953\n",
      "\n",
      "Test set: Average loss: 0.038993\n",
      "\n",
      "Train Epoch: 16 [    64/   214 (30%)]\tLearning rate: 0.0027\tLoss: 0.089422\n",
      "Train Epoch: 16 [   144/   214 (67%)]\tLearning rate: 0.0027\tLoss: 0.023530\n",
      "Train Epoch: 16 [   214/   214 (100%)]\tLearning rate: 0.0027\tLoss: 0.022510\n",
      "\n",
      "Test set: Average loss: 0.040366\n",
      "\n",
      "Train Epoch: 17 [    64/   214 (30%)]\tLearning rate: 0.0027\tLoss: 0.091639\n",
      "Train Epoch: 17 [   144/   214 (67%)]\tLearning rate: 0.0027\tLoss: 0.024394\n",
      "Train Epoch: 17 [   214/   214 (100%)]\tLearning rate: 0.0027\tLoss: 0.022855\n",
      "\n",
      "Test set: Average loss: 0.041262\n",
      "\n",
      "Train Epoch: 18 [    64/   214 (30%)]\tLearning rate: 0.0027\tLoss: 0.093626\n",
      "Train Epoch: 18 [   144/   214 (67%)]\tLearning rate: 0.0027\tLoss: 0.025179\n",
      "Train Epoch: 18 [   214/   214 (100%)]\tLearning rate: 0.0027\tLoss: 0.023318\n",
      "\n",
      "Test set: Average loss: 0.043907\n",
      "\n",
      "Train Epoch: 19 [    64/   214 (30%)]\tLearning rate: 0.0027\tLoss: 0.097244\n",
      "Train Epoch: 19 [   144/   214 (67%)]\tLearning rate: 0.0027\tLoss: 0.026944\n",
      "Train Epoch: 19 [   214/   214 (100%)]\tLearning rate: 0.0027\tLoss: 0.024294\n",
      "\n",
      "Test set: Average loss: 0.049222\n",
      "\n",
      "Train Epoch: 20 [    64/   214 (30%)]\tLearning rate: 0.0027\tLoss: 0.103092\n",
      "Train Epoch: 20 [   144/   214 (67%)]\tLearning rate: 0.0027\tLoss: 0.029748\n",
      "Train Epoch: 20 [   214/   214 (100%)]\tLearning rate: 0.0027\tLoss: 0.025913\n",
      "\n",
      "Test set: Average loss: 0.057206\n",
      "\n",
      "Train Epoch: 21 [    64/   214 (30%)]\tLearning rate: 0.0027\tLoss: 0.110728\n",
      "Train Epoch: 21 [   144/   214 (67%)]\tLearning rate: 0.0027\tLoss: 0.033900\n",
      "Train Epoch: 21 [   214/   214 (100%)]\tLearning rate: 0.0027\tLoss: 0.028388\n",
      "\n",
      "Test set: Average loss: 0.068723\n",
      "\n",
      "Train Epoch: 22 [    64/   214 (30%)]\tLearning rate: 0.0027\tLoss: 0.120627\n",
      "Train Epoch: 22 [   144/   214 (67%)]\tLearning rate: 0.0027\tLoss: 0.040103\n",
      "Train Epoch: 22 [   214/   214 (100%)]\tLearning rate: 0.0027\tLoss: 0.032249\n",
      "\n",
      "Test set: Average loss: 0.085067\n",
      "\n",
      "Train Epoch: 23 [    64/   214 (30%)]\tLearning rate: 0.0027\tLoss: 0.133243\n",
      "Train Epoch: 23 [   144/   214 (67%)]\tLearning rate: 0.0027\tLoss: 0.049441\n",
      "Train Epoch: 23 [   214/   214 (100%)]\tLearning rate: 0.0027\tLoss: 0.038381\n",
      "\n",
      "Test set: Average loss: 0.106931\n",
      "\n",
      "Train Epoch: 24 [    64/   214 (30%)]\tLearning rate: 0.0027\tLoss: 0.148729\n",
      "Train Epoch: 24 [   144/   214 (67%)]\tLearning rate: 0.0027\tLoss: 0.063539\n",
      "Train Epoch: 24 [   214/   214 (100%)]\tLearning rate: 0.0027\tLoss: 0.048144\n",
      "\n",
      "Test set: Average loss: 0.133284\n",
      "\n",
      "Train Epoch: 25 [    64/   214 (30%)]\tLearning rate: 0.0027\tLoss: 0.166596\n",
      "Train Epoch: 25 [   144/   214 (67%)]\tLearning rate: 0.0027\tLoss: 0.084772\n",
      "Train Epoch: 25 [   214/   214 (100%)]\tLearning rate: 0.0027\tLoss: 0.063472\n",
      "\n",
      "Test set: Average loss: 0.158415\n",
      "\n",
      "min loss found in ep  15  with minloss equals  0.038992512971162796 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = []\n",
    "lstoutput = []\n",
    "lstytest = []\n",
    "for ep in range(1, epochs+1):\n",
    "    train(ep)\n",
    "    Y_test, output, test_loss = evaluate()  \n",
    "    loss.append(test_loss.item())\n",
    "    lstoutput.append(output.tolist())\n",
    "    lstytest.append(Y_test.tolist())\n",
    "    \n",
    "minloss = min(loss)\n",
    "minidx = loss.index(minloss)\n",
    "\n",
    "Y_test = lstytest[minidx]\n",
    "output = lstoutput[minidx]\n",
    "    \n",
    "f, ax = plt.subplots()\n",
    "print(\"min loss found in ep \", minidx+1, \" with minloss equals \", minloss, \"\\n\")\n",
    "ax.scatter(Y_test, output,  color='black')\n",
    "ax.set_xlabel('Outputs from Marathon data')\n",
    "ax.set_ylabel('Outputs from predictions')\n",
    "ax.ticklabel_format(useOffset=False)\n",
    "plt.xlim(613.9, 614.8)\n",
    "plt.ylim(613.9, 614.8)\n",
    "figname = \"outerzone_tcn_opt\"\n",
    "ax.figure.savefig(figname, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55b65c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
