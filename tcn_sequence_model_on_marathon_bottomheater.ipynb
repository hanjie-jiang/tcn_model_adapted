{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50e33d78",
   "metadata": {},
   "source": [
    "### adapted from locuslab/TCN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe58b28",
   "metadata": {},
   "source": [
    "#### importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8530c02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import csv\n",
    "import re\n",
    "from matplotlib import cm\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_validate\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cd9c88",
   "metadata": {},
   "source": [
    "#### data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15bfae2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(path, pattern_interest): \n",
    "    # reading in the data\n",
    "    data = pd.read_csv(path,header=0)\n",
    "    # sorting data in dataframe\n",
    "    df1 = pd.DataFrame(data)\n",
    "    \n",
    "    #getting only the step 11 related variables\n",
    "    step11vars = [col for col in df1.columns if col.split('.')[0] == '11']\n",
    "    df2 = df1.loc[:, df1.columns.isin(step11vars)]\n",
    "    \n",
    "    # getting only the faceplate, bottom heater and heater outerzone related variables within the step11 dataset\n",
    "\n",
    "    # bottom heater related\n",
    "    bottom_related = [col for col in df2.columns if 'Bottom' in col]\n",
    "    dfb = df2.loc[:, df2.columns.isin(bottom_related)]\n",
    "\n",
    "    # faceplate related\n",
    "    faceplate_related = [col for col in df2.columns if \"Faceplate\" in col]\n",
    "    dff = df2.loc[:, df2.columns.isin(faceplate_related)]\n",
    "\n",
    "    # heater outer-zone related\n",
    "    outer_related = [col for col in df2.columns if \"HeaterOuter\" in col]\n",
    "    dfo = df2.loc[:, df2.columns.isin(outer_related)]\n",
    "    \n",
    "    # heater exchanger related\n",
    "    hx_related = [col for col in df2.columns if \"HX\" in col]\n",
    "    dfx = df2.loc[:, df2.columns.isin(hx_related)]\n",
    "    \n",
    "    # heater exchanger related\n",
    "    inner_related = [col for col in df2.columns if \"HeaterInner\" in col]\n",
    "    dfi = df2.loc[:, df2.columns.isin(inner_related)]\n",
    "    \n",
    "    # merge the three keywords related datasets together into `dfall`\n",
    "    dfall = pd.concat([dfb, dff, dfo, dfx, dfi], axis=1)\n",
    "    \n",
    "    mean_related = [col for col in dfall.columns if \"mean\" in col]\n",
    "    dfmean = dfall.loc[:, dfall.columns.isin(mean_related)]\n",
    "    \n",
    "    #Xbotmean = dfmean[dfmean.columns.drop(list(dfmean.filter(regex='11.FaceplateHeater_Temperature.mean')))]\n",
    "    Xmean = dfmean[dfmean.columns.drop(list(dfmean.filter(regex=pattern_interest)))]\n",
    "    ymean = dfmean.loc[:,[pattern_interest]]\n",
    "    \n",
    "    return Xmean, ymean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6af038f",
   "metadata": {},
   "source": [
    "#### TCN module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8719558e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import weight_norm\n",
    "\n",
    "\n",
    "class Chomp1d(nn.Module):\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size].contiguous()\n",
    "\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
    "                                 self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv2.weight.data.normal_(0, 0.01)\n",
    "        if self.downsample is not None:\n",
    "            self.downsample.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)\n",
    "\n",
    "\n",
    "class TemporalConvNet(nn.Module):\n",
    "    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n",
    "        super(TemporalConvNet, self).__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
    "                                     padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0719b979",
   "metadata": {},
   "source": [
    "#### upper level of the TCN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c579251",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TCN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_channels, kernel_size, dropout):\n",
    "        super(TCN, self).__init__()\n",
    "        self.tcn = TemporalConvNet(input_size, num_channels, kernel_size=kernel_size, dropout=dropout)\n",
    "        self.linear = nn.Linear(num_channels[-1], output_size)\n",
    "        self.init_weights() # if the data needs to be weighted\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.linear.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y1 = self.tcn(x)  # if multidimensional, input should have dimension (N, C, L)\n",
    "        o = self.linear(y1[:, :, -1])\n",
    "        return o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a91209",
   "metadata": {},
   "source": [
    "#### data generator for the adding problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45409e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "def data_generator(percent_train, seq_length):\n",
    "    \n",
    "    \"\"\"\n",
    "    Args:\n",
    "        seq_length: Length of the adding problem data\n",
    "        starting_index: the index of where the sequence starts; starting from 0\n",
    "    \"\"\"\n",
    "    # Specifying the feature columns that we need for predictions\n",
    "    col1 = '11.BottomHeater_Temperature.mean'\n",
    "    col2 = '11.FaceplateHeater_Temperature.mean'\n",
    "    col3 = '11.HeaterOuterZone_Temperature.mean'\n",
    "    col = [col1, col2, col3]\n",
    "    \n",
    "    # Giving the path to the data set\n",
    "    csv = \"../data/Marathon_data.csv\"\n",
    "    \n",
    "    # Data cleaning, getting the feature matrix and \n",
    "    X_bot, y_bot = data_cleaning(csv, col1)\n",
    "    \n",
    "    X_bot = torch.tensor(X_bot.to_numpy()).float()\n",
    "    y_bot = torch.tensor(y_bot.to_numpy()).float()\n",
    "    print(\"converted X_bot \", X_bot.shape, \"and y_bot \", y_bot.shape, \" to tensors\\n\")\n",
    "    \n",
    "    X_bot1 = torch.reshape(X_bot, [1, X_bot.shape[1], X_bot.shape[0]]) #[1,14,274]\n",
    "    y_bot1 = torch.reshape(y_bot, [1, y_bot.shape[0]]) #[1,274]\n",
    "\n",
    "    first_test_idx = int(round(percent_train * X_bot.shape[0]))\n",
    "    X_train = X_bot1[:, :, 0:first_test_idx]\n",
    "    X_test = X_bot1[:, :, first_test_idx:]\n",
    "    y_train = y_bot1[:, 0:first_test_idx]\n",
    "    y_test = y_bot1[:, first_test_idx:]\n",
    "    \n",
    "    X_train_final = torch.zeros([X_train.shape[2]-seq_length, X_train.shape[1], seq_length]) #[196, 14, 10] = [206-10, 14, 10]\n",
    "    y_train_final = torch.zeros([y_train.shape[1]-seq_length, 1]) #[196, 1] = [206-10, 1]\n",
    "    X_test_final = torch.zeros([X_test.shape[2]-seq_length, X_test.shape[1], seq_length]) #[58, 14, 10] = [68-10, 14, 10]\n",
    "    y_test_final = torch.zeros([y_test.shape[1]-seq_length, 1]) #[58,1] = [68-10, 1]\n",
    "    \n",
    "    for i in range(X_train.shape[2]-seq_length):\n",
    "        X_train_final[i, :, :] = X_train[: , : , i:i+seq_length]\n",
    "        y_train_final[i, :] = y_train[: , i+seq_length:i+seq_length+1]\n",
    "        \n",
    "    for i in range(X_test.shape[2]-seq_length):\n",
    "        X_test_final[i, :, :] = X_test[: , : , i:i+seq_length]\n",
    "        print(\"at index i = \", i, \"y_test[1, \", i+seq_length, \"] = \",y_test[: , i+seq_length:i+seq_length+1])\n",
    "        y_test_final[i, :] = y_test[: , i+seq_length:i+seq_length+1]\n",
    "        print(\"at index i = \", i, \"y_test_final[\",i, \", :] = \", y_test_final[i, :])\n",
    "\n",
    "    \n",
    "    print(\"Y_train shape is set to: \", y_train_final.shape, \"; Y_train type is: \", type(y_train_final), \"\\n\")\n",
    "    print(\"X_train shape is set to: \", X_train_final.shape, \"; X_train type is: \", type(X_train_final), \"\\n\")\n",
    "    print(\"Y_test shape is set to: \", y_test_final.shape, \"; Y_test type is: \", type(y_test_final), \"\\n\")\n",
    "    print(\"X_test shape is set to: \", X_test_final.shape, \"; X_test type is: \", type(X_test_final), \"\\n\")    \n",
    "    \n",
    "    return Variable(X_train_final), Variable(y_train_final), Variable(X_test_final), Variable(y_test_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d987e44",
   "metadata": {},
   "source": [
    "#### parameters needed for the program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "715f4970",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_channels = 14 # # of input features\n",
    "n_classes = 1  #size of each output sample; output feature sizes of y_pred\n",
    "batch_size = 32 #batch size defaults to 32\n",
    "seq_length = 10 #sequence length; defaults to 400\n",
    "epochs = 15 #upper epoch limit; defaults to 10\n",
    "clip = -1 #args.clip; gradient clip, -1 means no clip (default: -1)\n",
    "log_interval = 5 #args.log_interval; 'report interval (default: 100')\n",
    "channel_sizes = [30]*8 #[args.nhid]*args.levels; number of hidden units per layer * # of levels\n",
    "kernel_size = 7 #args.ksize; kernel_size\n",
    "dropout =  0.0 #args.dropout; dropouts applied to each layer; defaults to 0.0\n",
    "model = TCN(input_channels, n_classes, channel_sizes, kernel_size=kernel_size, dropout=dropout)\n",
    "lr = 4e-3 # learning rate is defaulted to be 0.0040\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr) #optimizer = getattr(optim, args.optim)(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efa65da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converted X_bot  torch.Size([274, 14]) and y_bot  torch.Size([274, 1])  to tensors\n",
      "\n",
      "at index i =  0 y_test[1,  10 ] =  tensor([[159.9098]])\n",
      "at index i =  0 y_test_final[ 0 , :] =  tensor([159.9098])\n",
      "at index i =  1 y_test[1,  11 ] =  tensor([[159.8806]])\n",
      "at index i =  1 y_test_final[ 1 , :] =  tensor([159.8806])\n",
      "at index i =  2 y_test[1,  12 ] =  tensor([[159.8912]])\n",
      "at index i =  2 y_test_final[ 2 , :] =  tensor([159.8912])\n",
      "at index i =  3 y_test[1,  13 ] =  tensor([[159.8919]])\n",
      "at index i =  3 y_test_final[ 3 , :] =  tensor([159.8919])\n",
      "at index i =  4 y_test[1,  14 ] =  tensor([[159.8957]])\n",
      "at index i =  4 y_test_final[ 4 , :] =  tensor([159.8957])\n",
      "at index i =  5 y_test[1,  15 ] =  tensor([[159.8935]])\n",
      "at index i =  5 y_test_final[ 5 , :] =  tensor([159.8935])\n",
      "at index i =  6 y_test[1,  16 ] =  tensor([[159.8955]])\n",
      "at index i =  6 y_test_final[ 6 , :] =  tensor([159.8955])\n",
      "at index i =  7 y_test[1,  17 ] =  tensor([[159.8977]])\n",
      "at index i =  7 y_test_final[ 7 , :] =  tensor([159.8977])\n",
      "at index i =  8 y_test[1,  18 ] =  tensor([[159.8950]])\n",
      "at index i =  8 y_test_final[ 8 , :] =  tensor([159.8950])\n",
      "at index i =  9 y_test[1,  19 ] =  tensor([[159.8844]])\n",
      "at index i =  9 y_test_final[ 9 , :] =  tensor([159.8844])\n",
      "at index i =  10 y_test[1,  20 ] =  tensor([[160.2374]])\n",
      "at index i =  10 y_test_final[ 10 , :] =  tensor([160.2374])\n",
      "at index i =  11 y_test[1,  21 ] =  tensor([[159.9222]])\n",
      "at index i =  11 y_test_final[ 11 , :] =  tensor([159.9222])\n",
      "at index i =  12 y_test[1,  22 ] =  tensor([[159.9025]])\n",
      "at index i =  12 y_test_final[ 12 , :] =  tensor([159.9025])\n",
      "at index i =  13 y_test[1,  23 ] =  tensor([[159.9065]])\n",
      "at index i =  13 y_test_final[ 13 , :] =  tensor([159.9065])\n",
      "at index i =  14 y_test[1,  24 ] =  tensor([[159.8952]])\n",
      "at index i =  14 y_test_final[ 14 , :] =  tensor([159.8952])\n",
      "at index i =  15 y_test[1,  25 ] =  tensor([[159.8927]])\n",
      "at index i =  15 y_test_final[ 15 , :] =  tensor([159.8927])\n",
      "at index i =  16 y_test[1,  26 ] =  tensor([[159.8831]])\n",
      "at index i =  16 y_test_final[ 16 , :] =  tensor([159.8831])\n",
      "at index i =  17 y_test[1,  27 ] =  tensor([[159.9025]])\n",
      "at index i =  17 y_test_final[ 17 , :] =  tensor([159.9025])\n",
      "at index i =  18 y_test[1,  28 ] =  tensor([[159.9088]])\n",
      "at index i =  18 y_test_final[ 18 , :] =  tensor([159.9088])\n",
      "at index i =  19 y_test[1,  29 ] =  tensor([[159.9443]])\n",
      "at index i =  19 y_test_final[ 19 , :] =  tensor([159.9443])\n",
      "at index i =  20 y_test[1,  30 ] =  tensor([[159.8982]])\n",
      "at index i =  20 y_test_final[ 20 , :] =  tensor([159.8982])\n",
      "at index i =  21 y_test[1,  31 ] =  tensor([[159.9053]])\n",
      "at index i =  21 y_test_final[ 21 , :] =  tensor([159.9053])\n",
      "at index i =  22 y_test[1,  32 ] =  tensor([[160.1783]])\n",
      "at index i =  22 y_test_final[ 22 , :] =  tensor([160.1783])\n",
      "at index i =  23 y_test[1,  33 ] =  tensor([[160.2096]])\n",
      "at index i =  23 y_test_final[ 23 , :] =  tensor([160.2096])\n",
      "at index i =  24 y_test[1,  34 ] =  tensor([[159.9025]])\n",
      "at index i =  24 y_test_final[ 24 , :] =  tensor([159.9025])\n",
      "at index i =  25 y_test[1,  35 ] =  tensor([[159.8972]])\n",
      "at index i =  25 y_test_final[ 25 , :] =  tensor([159.8972])\n",
      "at index i =  26 y_test[1,  36 ] =  tensor([[159.9038]])\n",
      "at index i =  26 y_test_final[ 26 , :] =  tensor([159.9038])\n",
      "at index i =  27 y_test[1,  37 ] =  tensor([[159.8987]])\n",
      "at index i =  27 y_test_final[ 27 , :] =  tensor([159.8987])\n",
      "at index i =  28 y_test[1,  38 ] =  tensor([[159.8995]])\n",
      "at index i =  28 y_test_final[ 28 , :] =  tensor([159.8995])\n",
      "at index i =  29 y_test[1,  39 ] =  tensor([[159.8937]])\n",
      "at index i =  29 y_test_final[ 29 , :] =  tensor([159.8937])\n",
      "at index i =  30 y_test[1,  40 ] =  tensor([[159.9015]])\n",
      "at index i =  30 y_test_final[ 30 , :] =  tensor([159.9015])\n",
      "at index i =  31 y_test[1,  41 ] =  tensor([[159.8937]])\n",
      "at index i =  31 y_test_final[ 31 , :] =  tensor([159.8937])\n",
      "at index i =  32 y_test[1,  42 ] =  tensor([[159.8922]])\n",
      "at index i =  32 y_test_final[ 32 , :] =  tensor([159.8922])\n",
      "at index i =  33 y_test[1,  43 ] =  tensor([[159.9045]])\n",
      "at index i =  33 y_test_final[ 33 , :] =  tensor([159.9045])\n",
      "at index i =  34 y_test[1,  44 ] =  tensor([[159.8919]])\n",
      "at index i =  34 y_test_final[ 34 , :] =  tensor([159.8919])\n",
      "at index i =  35 y_test[1,  45 ] =  tensor([[159.8891]])\n",
      "at index i =  35 y_test_final[ 35 , :] =  tensor([159.8891])\n",
      "at index i =  36 y_test[1,  46 ] =  tensor([[159.9161]])\n",
      "at index i =  36 y_test_final[ 36 , :] =  tensor([159.9161])\n",
      "at index i =  37 y_test[1,  47 ] =  tensor([[159.8909]])\n",
      "at index i =  37 y_test_final[ 37 , :] =  tensor([159.8909])\n",
      "at index i =  38 y_test[1,  48 ] =  tensor([[159.8960]])\n",
      "at index i =  38 y_test_final[ 38 , :] =  tensor([159.8960])\n",
      "at index i =  39 y_test[1,  49 ] =  tensor([[159.9151]])\n",
      "at index i =  39 y_test_final[ 39 , :] =  tensor([159.9151])\n",
      "at index i =  40 y_test[1,  50 ] =  tensor([[159.9159]])\n",
      "at index i =  40 y_test_final[ 40 , :] =  tensor([159.9159])\n",
      "at index i =  41 y_test[1,  51 ] =  tensor([[159.9096]])\n",
      "at index i =  41 y_test_final[ 41 , :] =  tensor([159.9096])\n",
      "at index i =  42 y_test[1,  52 ] =  tensor([[159.8995]])\n",
      "at index i =  42 y_test_final[ 42 , :] =  tensor([159.8995])\n",
      "at index i =  43 y_test[1,  53 ] =  tensor([[159.8932]])\n",
      "at index i =  43 y_test_final[ 43 , :] =  tensor([159.8932])\n",
      "at index i =  44 y_test[1,  54 ] =  tensor([[159.9146]])\n",
      "at index i =  44 y_test_final[ 44 , :] =  tensor([159.9146])\n",
      "at index i =  45 y_test[1,  55 ] =  tensor([[159.9091]])\n",
      "at index i =  45 y_test_final[ 45 , :] =  tensor([159.9091])\n",
      "at index i =  46 y_test[1,  56 ] =  tensor([[159.8985]])\n",
      "at index i =  46 y_test_final[ 46 , :] =  tensor([159.8985])\n",
      "at index i =  47 y_test[1,  57 ] =  tensor([[160.1907]])\n",
      "at index i =  47 y_test_final[ 47 , :] =  tensor([160.1907])\n",
      "at index i =  48 y_test[1,  58 ] =  tensor([[160.1934]])\n",
      "at index i =  48 y_test_final[ 48 , :] =  tensor([160.1934])\n",
      "at index i =  49 y_test[1,  59 ] =  tensor([[159.8990]])\n",
      "at index i =  49 y_test_final[ 49 , :] =  tensor([159.8990])\n",
      "at index i =  50 y_test[1,  60 ] =  tensor([[159.8945]])\n",
      "at index i =  50 y_test_final[ 50 , :] =  tensor([159.8945])\n",
      "at index i =  51 y_test[1,  61 ] =  tensor([[159.9000]])\n",
      "at index i =  51 y_test_final[ 51 , :] =  tensor([159.9000])\n",
      "at index i =  52 y_test[1,  62 ] =  tensor([[159.8892]])\n",
      "at index i =  52 y_test_final[ 52 , :] =  tensor([159.8892])\n",
      "at index i =  53 y_test[1,  63 ] =  tensor([[159.8987]])\n",
      "at index i =  53 y_test_final[ 53 , :] =  tensor([159.8987])\n",
      "at index i =  54 y_test[1,  64 ] =  tensor([[159.8912]])\n",
      "at index i =  54 y_test_final[ 54 , :] =  tensor([159.8912])\n",
      "at index i =  55 y_test[1,  65 ] =  tensor([[159.8990]])\n",
      "at index i =  55 y_test_final[ 55 , :] =  tensor([159.8990])\n",
      "at index i =  56 y_test[1,  66 ] =  tensor([[159.8967]])\n",
      "at index i =  56 y_test_final[ 56 , :] =  tensor([159.8967])\n",
      "at index i =  57 y_test[1,  67 ] =  tensor([[159.8940]])\n",
      "at index i =  57 y_test_final[ 57 , :] =  tensor([159.8940])\n",
      "at index i =  58 y_test[1,  68 ] =  tensor([[159.9023]])\n",
      "at index i =  58 y_test_final[ 58 , :] =  tensor([159.9023])\n",
      "at index i =  59 y_test[1,  69 ] =  tensor([[159.8887]])\n",
      "at index i =  59 y_test_final[ 59 , :] =  tensor([159.8887])\n",
      "at index i =  60 y_test[1,  70 ] =  tensor([[159.8902]])\n",
      "at index i =  60 y_test_final[ 60 , :] =  tensor([159.8902])\n",
      "at index i =  61 y_test[1,  71 ] =  tensor([[159.8945]])\n",
      "at index i =  61 y_test_final[ 61 , :] =  tensor([159.8945])\n",
      "at index i =  62 y_test[1,  72 ] =  tensor([[159.8834]])\n",
      "at index i =  62 y_test_final[ 62 , :] =  tensor([159.8834])\n",
      "at index i =  63 y_test[1,  73 ] =  tensor([[159.8917]])\n",
      "at index i =  63 y_test_final[ 63 , :] =  tensor([159.8917])\n",
      "at index i =  64 y_test[1,  74 ] =  tensor([[159.8846]])\n",
      "at index i =  64 y_test_final[ 64 , :] =  tensor([159.8846])\n",
      "at index i =  65 y_test[1,  75 ] =  tensor([[159.9116]])\n",
      "at index i =  65 y_test_final[ 65 , :] =  tensor([159.9116])\n",
      "at index i =  66 y_test[1,  76 ] =  tensor([[159.8919]])\n",
      "at index i =  66 y_test_final[ 66 , :] =  tensor([159.8919])\n",
      "at index i =  67 y_test[1,  77 ] =  tensor([[159.8891]])\n",
      "at index i =  67 y_test_final[ 67 , :] =  tensor([159.8891])\n",
      "at index i =  68 y_test[1,  78 ] =  tensor([[159.8955]])\n",
      "at index i =  68 y_test_final[ 68 , :] =  tensor([159.8955])\n",
      "at index i =  69 y_test[1,  79 ] =  tensor([[159.8950]])\n",
      "at index i =  69 y_test_final[ 69 , :] =  tensor([159.8950])\n",
      "at index i =  70 y_test[1,  80 ] =  tensor([[159.8887]])\n",
      "at index i =  70 y_test_final[ 70 , :] =  tensor([159.8887])\n",
      "at index i =  71 y_test[1,  81 ] =  tensor([[159.8947]])\n",
      "at index i =  71 y_test_final[ 71 , :] =  tensor([159.8947])\n",
      "Y_train shape is set to:  torch.Size([182, 1]) ; Y_train type is:  <class 'torch.Tensor'> \n",
      "\n",
      "X_train shape is set to:  torch.Size([182, 14, 10]) ; X_train type is:  <class 'torch.Tensor'> \n",
      "\n",
      "Y_test shape is set to:  torch.Size([72, 1]) ; Y_test type is:  <class 'torch.Tensor'> \n",
      "\n",
      "X_test shape is set to:  torch.Size([72, 14, 10]) ; X_test type is:  <class 'torch.Tensor'> \n",
      "\n",
      "torch.Size([182, 14, 10]) torch.Size([182, 1]) torch.Size([72, 14, 10]) torch.Size([72, 1])\n"
     ]
    }
   ],
   "source": [
    "percent_train = 0.7\n",
    "X_train, Y_train, X_test, Y_test = data_generator(percent_train,seq_length)\n",
    "print(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e784c8c3",
   "metadata": {},
   "source": [
    "#### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce8873bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    \n",
    "    global lr\n",
    "    model.train()\n",
    "    batch_idx = 1\n",
    "    total_loss = 0\n",
    "    for i in range(0, X_train.size(0), batch_size):\n",
    "        if i + batch_size > X_train.size(0):\n",
    "            x, y = X_train[i:], Y_train[i:]\n",
    "        else:\n",
    "            x, y = X_train[i:(i+batch_size)], Y_train[i:(i+batch_size)]\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        loss = torch.nn.functional.mse_loss(output, y)\n",
    "        loss.backward()\n",
    "        if clip > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        batch_idx += 1\n",
    "        total_loss += loss.item()\n",
    "        print(total_loss)\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            processed = min(i+batch_size, X_train.size(0))\n",
    "            print('Train Epoch: {:2d} [{:6d}/{:6d} ({:.0f}%)]\\tLearning rate: {:.4f}\\tLoss: {:.6f}'.format(\n",
    "                epoch, processed, X_train.size(0), 100.*processed/X_train.size(0), lr, cur_loss))\n",
    "            total_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0e5073",
   "metadata": {},
   "source": [
    "#### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55ae7c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(X_test)\n",
    "        test_loss = torch.nn.functional.mse_loss(output, Y_test)\n",
    "        print('\\nTest set: Average loss: {:.6f}\\n'.format(test_loss.item()))\n",
    "        return Y_test, output, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f24f572d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25668.529296875\n",
      "44234.65625\n",
      "53541.2314453125\n",
      "53824.20587158203\n",
      "Train Epoch:  1 [   128/   182 (70%)]\tLearning rate: 0.0040\tLoss: 10764.841174\n",
      "19233.65625\n",
      "22488.956298828125\n",
      "\n",
      "Test set: Average loss: 123.839371\n",
      "\n",
      "130.25372314453125\n",
      "2381.6834106445312\n",
      "6805.254211425781\n",
      "12367.936828613281\n",
      "Train Epoch:  2 [   128/   182 (70%)]\tLearning rate: 0.0040\tLoss: 2473.587366\n",
      "5720.19091796875\n",
      "10724.98095703125\n",
      "\n",
      "Test set: Average loss: 3762.114990\n",
      "\n",
      "3808.58984375\n",
      "6037.946044921875\n",
      "6860.238342285156\n",
      "6915.869468688965\n",
      "Train Epoch:  3 [   128/   182 (70%)]\tLearning rate: 0.0040\tLoss: 1383.173894\n",
      "334.18701171875\n",
      "1692.625\n",
      "\n",
      "Test set: Average loss: 2076.143799\n",
      "\n",
      "2041.1708984375\n",
      "3952.281982421875\n",
      "5087.094482421875\n",
      "5483.682098388672\n",
      "Train Epoch:  4 [   128/   182 (70%)]\tLearning rate: 0.0040\tLoss: 1096.736420\n",
      "33.62425994873047\n",
      "105.56099700927734\n",
      "\n",
      "Test set: Average loss: 330.511963\n",
      "\n",
      "342.8065185546875\n",
      "952.8850708007812\n",
      "1715.9352416992188\n",
      "2464.7876586914062\n",
      "Train Epoch:  5 [   128/   182 (70%)]\tLearning rate: 0.0040\tLoss: 492.957532\n",
      "599.0645141601562\n",
      "955.8725280761719\n",
      "\n",
      "Test set: Average loss: 138.545731\n",
      "\n",
      "146.810546875\n",
      "163.28416061401367\n",
      "188.8649196624756\n",
      "332.3006191253662\n",
      "Train Epoch:  6 [   128/   182 (70%)]\tLearning rate: 0.0040\tLoss: 66.460124\n",
      "273.83892822265625\n",
      "608.2774658203125\n",
      "\n",
      "Test set: Average loss: 285.558624\n",
      "\n",
      "274.9065856933594\n",
      "441.01649475097656\n",
      "497.82040786743164\n",
      "502.379355430603\n",
      "Train Epoch:  7 [   128/   182 (70%)]\tLearning rate: 0.0040\tLoss: 100.475871\n",
      "18.025381088256836\n",
      "87.20418357849121\n",
      "\n",
      "Test set: Average loss: 121.377945\n",
      "\n",
      "129.30343627929688\n",
      "277.6716003417969\n",
      "405.7814636230469\n",
      "489.79185485839844\n",
      "Train Epoch:  8 [   128/   182 (70%)]\tLearning rate: 0.0040\tLoss: 97.958371\n",
      "35.52281188964844\n",
      "41.09423875808716\n",
      "\n",
      "Test set: Average loss: 5.732716\n",
      "\n",
      "4.899742603302002\n",
      "32.80207681655884\n",
      "88.78071451187134\n",
      "157.02741861343384\n",
      "Train Epoch:  9 [   128/   182 (70%)]\tLearning rate: 0.0040\tLoss: 31.405484\n",
      "57.589454650878906\n",
      "91.22615432739258\n",
      "\n",
      "Test set: Average loss: 10.892354\n",
      "\n",
      "9.266611099243164\n",
      "11.2348552942276\n",
      "18.154694199562073\n",
      "37.842201828956604\n",
      "Train Epoch: 10 [   128/   182 (70%)]\tLearning rate: 0.0040\tLoss: 7.568440\n",
      "28.93661117553711\n",
      "58.719045639038086\n",
      "\n",
      "Test set: Average loss: 21.751150\n",
      "\n",
      "25.249340057373047\n",
      "37.506099700927734\n",
      "40.20823788642883\n",
      "42.197503328323364\n",
      "Train Epoch: 11 [   128/   182 (70%)]\tLearning rate: 0.0040\tLoss: 8.439501\n",
      "7.055729389190674\n",
      "20.6814923286438\n",
      "\n",
      "Test set: Average loss: 16.251390\n",
      "\n",
      "13.974740982055664\n",
      "26.5148868560791\n",
      "34.254847049713135\n",
      "37.08309721946716\n",
      "Train Epoch: 12 [   128/   182 (70%)]\tLearning rate: 0.0040\tLoss: 7.416619\n",
      "1.106127381324768\n",
      "4.1151779890060425\n",
      "\n",
      "Test set: Average loss: 5.745262\n",
      "\n",
      "7.58528995513916\n",
      "16.447782516479492\n",
      "23.07659387588501\n",
      "27.429340362548828\n",
      "Train Epoch: 13 [   128/   182 (70%)]\tLearning rate: 0.0040\tLoss: 5.485868\n",
      "1.7264937162399292\n",
      "2.839974880218506\n",
      "\n",
      "Test set: Average loss: 1.983991\n",
      "\n",
      "1.6503065824508667\n",
      "4.94269073009491\n",
      "9.528220534324646\n",
      "13.711301684379578\n",
      "Train Epoch: 14 [   128/   182 (70%)]\tLearning rate: 0.0040\tLoss: 2.742260\n",
      "2.8425302505493164\n",
      "4.320661306381226\n",
      "\n",
      "Test set: Average loss: 0.853576\n",
      "\n",
      "1.2071726322174072\n",
      "3.0720436573028564\n",
      "4.994779586791992\n",
      "7.372563123703003\n",
      "Train Epoch: 15 [   128/   182 (70%)]\tLearning rate: 0.0040\tLoss: 1.474513\n",
      "1.8983229398727417\n",
      "3.289544463157654\n",
      "\n",
      "Test set: Average loss: 0.817687\n",
      "\n",
      "min loss found in ep  14 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZwAAAEGCAYAAABRvCMcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAApiUlEQVR4nO3df5hdVX3v8fdnJolyEBQmgWJhziAXvAWF1ESLVtuARWmuFWmxtR01XriOBn9QWnkKnduK2nlEfhRzawN3wISUmaJoqSJSqaJARStOFEIAKf6YicFcEqAKmIpk5nv/2PuEM5PzY8/M2WfOmfm8nmc/Z5919tr7m52Z+Z6199prKSIwMzPLW8dcB2BmZguDE46ZmTWFE46ZmTWFE46ZmTWFE46ZmTXForkOoBmWLl0aPT09cx2GmVlb2bx586MRsaxR+1sQCaenp4eRkZG5DsPMrK1IGmvk/nxJzczMmiK3hCNpg6SdkrZOKX+fpAcl3Sfp4rSsS9LXJD0l6RM19nmwpC9Leih9PSiv+M3MrLHybOFcA5xaXiDpJOA04PiIOA64NP3oF8BfAR+os8/zgVsj4mjg1vS9mZm1gdwSTkTcATw+pXgtcFFEPJ1uszN9/XlEfJ0k8dRyGrApXd8EvKlhAZuZWa6afQ/nGOA1kr4l6XZJL59m/UMjYgdA+npItQ0l9UkakTSya9euWYRsZmaN0OyEswg4CDgROA+4XpLyOFBEDEbEyohYuWzZ7Hv1DQ8P09PTQ0dHBz09PQwPDzcgSjNrJf49z1ezE8524IZI3AVMAEunUf8RSYcBpK87c4hxH8PDw/T19TE2NkZEMDY2Rl9f36Qfxkb9oPoH3mxuZPk9t1mKiNwWoAfYWvb+3cCH0/VjgB8DKvv8HcAnauzvEuD8dP184OIscaxYsSJmo1gsBrDPUiwWIyJiaGgoCoXCpM8KhUIMDQ1N6ziN2o+ZTV+93/OFCBiJBuYERU7z4Ui6DlhF0oJ5BPggcC2wAVgO/BL4QER8Nd1+FDgQWAL8FHhdRNwv6WrgyogYkdQFXA90A9uAN0fE1I4J+1i5cmXM5sHPjo4OKp0nSUxMTNDT08PY2L7PRxWLRUZHRzMfp1H7MbPpq/d7vhBJ2hwRKxu2v7wSTiuZbcKplwhq3Yaazvn1D7zZ3PEXvn01OuF4pIEMBgYGKBQKk8oKhQIDAwMAdHZ2VqxXrbya7u7uaZWbWePU+z232XPCyaC3t5fBwUGKxSKSKBaLDA4O0tvbC8D4+HjFetXKq/EPvNncqfd7brPnS2oN0Mim+PDwMP39/Wzbto3u7m4GBgb8A29mc8KX1FpQI1smvb29jI6OMjExwejoqJONmc0bTjgN4Ka4mVl9vqRmZmYV+ZKamZm1JSccMzNrCiecGag13lnpM0ksWrQISdMaE63avj3Gmpm1vUaOk9Oqy2zHUitXa7yzSp9N3WYm+167dq3HWDOzpqNdxlJrJY3sNFDrmRug4mfl29R6Lqfavjs7Oys+RLqQh9wws/w1utPAokbtaKHYtm3btMqns021z6uNWJDlmGZmrcL3cKap1nhn9cY8m+nn1cZk8xhrZtZOnHCmqdaoApU+m7rNTPbd19fnMdbMrP018oZQqy6N7DQQkdzcLxaLISmKxeKkm/elz4Do7OzcO4FT1hv81fZd65hmZnnAnQamzyMNmJlNn0caMDOztuSEY2ZmTeGEY2ZmTeGEY2ZmTeGEY2ZmTeGEY2ZmTZFbwpG0QdJOSVunlL9P0oOS7pN0cVn5BZK+n372+ir7vFDSw5LuTpfVecVvZmaNlWcL5xrg1PICSScBpwHHR8RxwKVp+bHAW4Dj0jrrJVUezwUuj4jl6XJzXsGbmbWzVpzSJLeEExF3AI9PKV4LXBQRT6fb7EzLTwM+FRFPR8SPgO8Dr8grtrx4LhszawXDw8P09fUxNjZGRDA2NkZfX9/c/+1p5LAFUxegB9ha9v5u4EPAt4DbgZen5Z8A3lq23SeBMyrs70JgFNgCbAAOqnHsPmAEGOnu7p7N6A4VVRrCRpLnsjGzOVf62zR1KRaL09oPDR7aptmdBhYBBwEnAucB10sSoArbVhpz5wrgKGA5sAO4rNqBImIwIlZGxMply5bNNu5Jyr89wLPTB8SUYYJ2797N4OAgu3fv3qe8v7+/oTGZmZXMZhqVPDU74WwHbkiT513ABLA0LT+ibLvDgZ9MrRwRj0TEeERMAFcxR5fd+vv790ki1XguGzNrtlrTqMylZieczwEnA0g6BlgCPArcCLxF0nMkHQkcDdw1tbKkw8reng5snbpNM0wnWXguGzNrtlrTqMylPLtFXwd8E3ixpO2SziK57/KitKv0p4A1aWvnPuB64H7gS8B7ImI83c/VkkqjlV4s6V5JW4CTgHPzir+WrMnCc9mY2Vzo7e1lcHCQYrGIJIrFIoODg/T29s5tYI28IdSqSx7z4UztCFBaSh0HPJeNmbU7PB/O9OUxH87w8DD9/f2MjY3R2dnJ+Pg4xWKRgYGBuf8WYWbWAJ4Pp8VI4vDDD2doaIiBgQH6+/szPW/jZ3PMbKFxC2cGSt2iy3uqLVmyhIjgmWeembRtV1cX69atm9TqqVS/UCi0xjVWM7NUo1s4Tjgz0NPTs/cZnCymJpNq9YvFIqOjo40K08xsVpxwZqDRCaejo2OfhzzrKU8m1epLYmJiohEhmpnNmu/htICZPENT/uxOqz6UZWaWp7oJR9LFkg6UtFjSrZIelfTWZgTXqio9VFVPeTJp1YeyzMzylKWF87qIeAJ4A8kQNMeQjIO2YJUeqqo2isBUU5NJyz6UZWaWo0UZtlmcvq4GrouIx5PxNhe2UnKY2ttMEhFR99mc3t5eJxgzW1CytHC+IOl7wErgVknLgF/kG1Z7KG+pwLPJBpJBO0stm2qJxc/imNlCkqmXmqSDgCciYlxSATgwIv5f7tE1SB4jDUxVq6t0Z2cnfX19rF+/fm+Zn8Uxs1Y3V73Ufg34I0lvB84AXteoANpRpZZJrRGkx8fHueKKKzj77LP3llWa4sDz5JjZfFa3hSPpWpJJz+4GSpO7RES8P9/QGqeRLZxqLZP99tuPxx57rGbdzs5O9uzZA/hZHDNrfY1u4WTpNLASODYWwhOiGVRrmey3335165ZPxtbd3V3xEpyfxTGz+SrLJbWtwK/kHUi7qHbp7PHHH6erq6tm3fJu1H4Wx8wWmiwJZylwv6RbJN1YWvIOrFXVGiVg3bp1NR8I7evr27vuZ3HMbKHJcg/ntyuVR8TtuUSUg5ncwynNd7Nt2za6u7v3dm+udA+n1B26WCyyevVqbr755kmXyyr1UjMza3VNv4cTEbdLOhR4eVp0V0TsbFQArWhqUhkbG9vbOim1QEqTr5U/ezM2NsamTZvcUjEzqyBLC+cPgUuA2wABrwHOi4jP5h5dg0y3hZN1+gBPM2Bm89lcPIfTD7w8ItZExNuBVwB/1agAWlG1jgFTy6ttNzY25lEDzMymyJJwOqZcQnssY722lXX6gFpdmN/+9rc76ZiZlcmSOL6U9lB7h6R3AF8Ebq5XSdIGSTslbZ1S/j5JD0q6T9LFZeUXSPp++tnrq+zzYElflvRQ+npQhvinLWuX5VpdmCcmJjjnnHPyCM/MrC3VTTgRcR4wCBwPnAAMRsRfZNj3NcCp5QWSTgJOA46PiOOAS9PyY4G3AMelddZLqjT2//nArRFxNHBr+r7hsnZZrtcxoN7IA2ZmC0muU0xL6gFuioiXpO+vJ0lYX5my3QUAEfHR9P0twIUR8c0p2z0IrIqIHZIOA26LiBfXiyPPwTsXLVo0aQSBqTxAg5m1q6Z1GpD09fT1SUlPlC1PSnpihsc7BniNpG9Jul1Sqav1rwI/Lttue1o21aERsQMgfT2kRvx9kkYkjezatWuG4dZX/jDnVPVGHjAzW0iqJpyIeHX6ekBEHFi2HBARB87weIuAg4ATSWYNvV7JbG6VZnSbVdMgIgYjYmVErFy2bNlsdlXT+vXree1rX7tP+eLFi1m3bl1uxzUzazd17+Gko0XXLctoO3BDJO4CJkiGztkOHFG23eHATyrUfyS9lEb62hIPoH7lK19haGho0j2fjRs3+uFPM7MyWXqpHVf+RtIiYMUMj/c54OR0P8cAS4BHgRuBt0h6jqQjgaOBuyrUvxFYk66vAT4/wzgapjQ3ztve9jYArr32WkZHRysmG8/waWYLWkRUXIALgCeBPcAT6fIkyXM4H61Wr6z+dcAO4BmSFsxZJAlmiGQE6u8AJ5dt3w/8AHgQ+N2y8quBlel6F0nvtIfS14PrxRERrFixIvIwNDQUhUIhSC7/BRCFQiHWrl0bXV1de8u6urpi7dq1FbcdGhrKJTYzs9kCRiLD39isS5ahbT4aERfMJqnNtUb3UisN7FltSunp8DA4Ztaq5mJom7skPb8sgBdIelOjAmg3pYE9G5FsoPrwOGZm802WhPPBiPhZ6U1E/BT4YG4RtbhKM37Ohmf4NLOFItNYahXKskxNPS/Va5Ekvbyz8QyfZraQZEk4I5L+VtJRkl4k6XJgc96BtapaLZKuri7233//TPvZf//9Kw6X455sZjZfZUk47wN+CXwa+AzwC+A9eQbVyioN7AlJAnnyySd56qmnMu2n0mW58vtDEbF34jcnHTObD3IdS61V5NFL7Zxzzpn14Jye0M3MWlmje6lVTTiSPh4RfyrpC1QYZiYi3tioIPKWx+Cd1ZLDdEhiYmJi7/uOjo6Kg31O3c7MrBkanXBq3fwvDV9zaaMONp9k7c7c1dXF448/XjGRVJrQrVISc082M5sPag3euTl9vb3S0rwQW1OWJFAoFFi3bh3vfve79+m9Vm1CtywTv5mZtaNa0xPcK2lLtaWZQbaiSslh8eLFdHV17TNp2/r167n22mszTeiWZeI3M7N2VOseTjFdLfVIK11i6wV2R8SHc46tYfKagK00xM22bdvo7u5mYGDAycHM5o2mdRooO+CdEfGb9cpaWZ4zfpqZzVdzMZba/pJeXRbAq4BsTzeamZmlsgxRcxawIR3AM4CfAWfmGpWZmc07dRNO2lvtBEkHklyC+1m9OmZmZlNlmWL6UEmfBD4dET+TdKyks5oQm5mZzSNZ7uFcA9wCvDB9/x/An+YUT9vyoJtmZrVlSThLI+J6YAIgIvYA47lG1WY86KbZwuMvmdOXJeH8XFIX6Xhqkk4k6ThgqUqTsu3evZv+/v5JZa3yA9oqcZi1K3/JnKGIqLkALwPuJEkyd5JcUju+Xr1WWlasWBF5khQkCXnSImnvNkNDQ1EoFCZ9XigUYmhoKNfYpmqVOMzaWbFYrPg7XywW5zq0hgJGooF/i2s++CmpE3g/8HfAiwEBD0bEM41Ne/nK+8HPLNMKtMrUA60Sh1k7Wygjuzf1wc+IGAdOi4g9EXFfRGzNmmwkbZC0U9LWsrILJT0s6e50WZ2WL5G0MR2/7R5Jq6rss2L9ubZ69eq6g3NWG10666jTjdIqcZi1s2qD93pk99qy3MO5U9InJL1G0stKS4Z61wCnVii/PCKWp8vNadk7ASLipcApwGWSqsVWqf6cGR4eZtOmTZO+7UhizZo1k8ZVa5Uf0FaJw6ydeWT3mcmScF4FHAd8GLgsXerOkRMRdwCPZ4zjWODWtN5O4KdAw5pxearUYSAiuPnmybmwVX5AWyUOs3bmkd1nqJE3hKYuQA+wtez9hcAosAXYAByUlvcBnyEZ+eBIkoTzBxX2V7F+vSXPTgNZOgyUDA0NRbFYDElRLBbn7EZ9q8RhZq2NZnYaAEi7RH8QeHX6x/TrwIcj4rF6yUxSD3BTRLwkfX8o8Gi6n48Ah0XEmZIWAZcAJwFjwGLg/0bE56fsr2L9KsfuSxMZ3d3dK2Y7HXQ1vglvZvPVXIwW/SlgF/AHwBnp+qdncrCIeCQixiNiArgKeEVavicizo3kvsxpwAuAh7LWr3KswYhYGRErly1bNpNwM/ElKjOzbLIknIMj4iMR8aN0+RuShDBtkg4re3s6sDUtL0jaP10/BdgTEfdnrT+XfC3XzCybLNMTfE3SW4Dr0/dnAF+sV0nSdcAqYKmk7SSX5VZJWk5ySWwUeFe6+SHALZImgIeBt5Xt52rgyogYAS6uUn9O9fb2OsGYmdWR5R7OkyQTrpXGT+sEfp6uR0QcmF94jeEZP83Mpq/R93CyzIdzQKMOZmZmC1eWezhmZmaz5oRjZmZN4YRjZmZNkaWXGpIOAo4o3z4ivpNXUGZmNv/UTTiSPgK8A/gB6SRs6evJ+YVlZmbzTZYWzh8CR0XEL/MOxszM5q8s93C2MsORBczMzEqytHA+Cnw3nUjt6VJhRLwxt6jMzGzeyZJwNgEfA+4F5s/cqWZm1lRZEs6jEfF/co/EzMzmtSz3cDZL+qikV05ziul5Y3h4mJ6eHjo6Oujp6eHss8+mp6cHSSxatAhJ9PT0MDw8nKn+8PBwxTIzs3mt3gxtwNcqLF9t5CxweS+zmfFzaGgoCoVCxVk9py6FQmHS7JlDQ0PR1dW1z3aLFy+OJUuW7DNDKOAZOM0WiHaYeZcGz/g558mgGctsEk6xWMyUbEpLV1dXREwvUdVLXGY2v1T6+9CKv/eNTjhZpid4PslcNr+VFt1OMsX0z7K3o+bWbKYn6OjooN45mmrt2rXcfPPNFaeezspTVJvNX+0yNX2jpyfIknD+ieRZnE1p0duAEyLi9xsVRN5mk3Cq/WDUImnaSarSPiYm3CnQbD6q9kW21X7vG51wsnQaOCoiPhgRP0yXDwEvalQArW5gYIBCoTCtOhFBZ2dn1c8XL17MkiVLau6ju7t7Wsc0s/ZR7fd7vv/eZ0k4/yXp1aU3kn4T+K/8Qmotvb29DA4OUiwWkUSxWGTt2rUUi8Wa9cbHxysmqq6uLjZu3MiGDRv27kPSpG0KhQIDAwON+0eYWUup9EV2Qfze17vJA5wA3AOMpst3geMbeSMp72U2nQZqGRoa2tu7bOpS6nWSpRdKO/RWMbPGaoffe5rZaUBSJ3BRRJwn6cA0QT2RW/bLyWzu4dRz9tlnc+WVV066HlsoFBgcHKS3tzeXY5qZNUNT7+FExDiwIl1/oh2TTd7Wr1/PtddeO+mSm5ONmdm+svRSuww4GvgM8PNSeUTckG9ojZNnC8fMbL6ai15qBwOPkUy49nvp8oZ6lSRtkLQzHWW6VHahpIcl3Z0uq9PyJZI2SrpX0j2SVlXZ58GSvizpofT1oAzxm5lZC6iacCR9LF29OSL+55TlzAz7vgY4tUL55RGxPF1uTsveCRARLwVOAS6TVCm284FbI+Jo4Nb0vZmZtYFaLZzVkhYDF8xkxxFxB/B4xs2PJUkgRMRO4KdApWbcaTz7AOom4E0zic3MzJqvVsL5EvAocLykJ8qWJyXNpvPAeyVtSS+5lS6J3QOcJmmRpCNJOiocUaHuoRGxAyB9PaTaQST1SRqRNLJr165ZhGtmZo1QNeFExHkR8XzgixFxYNlyQEQcOMPjXQEcBSwHdgCXpeUbgO3ACPBx4BvAnhkeoxT/YESsjIiVy5Ytm82uzMysAepOwBYRpzXqYBHxSGld0lXATWn5HuDcss++ATxUYRePSDosInZIOgzY2ajYzMwsX1l6qTVMmiRKTicZFBRJBUn7p+unAHsi4v4Ku7gRWJOurwE+n2O4ZmbWQFmmmJ4RSdcBq4ClkraTTHGwStJykuFfRoF3pZsfAtwiaQJ4mGRE6tJ+rgaujIgR4CLgeklnAduAN+cVv5mZNVbdBz8nbZzc5D8iIrbkF1Lj+cFPM7Ppa/qDn5Juk3SgpINJepNtlPS3jQrAzMwWhiz3cJ6fjqH2+8DGiFgB/E6+YZmZ2XyTJeEsSm/2/yFprzIzM7PpypJwPgTcAnw/Ir4t6UVU7rJsZmZWVZZeajsi4vjSm4j4oe/hmJnZdGVp4fxdxjIzM7OqqrZwJL0SeBWwTNKflX10INCZd2BmZja/1LqktgR4XrrNAWXlTwBn5BmUmZnNP1UTTkTcDtwu6ZqIGGtiTGZmNg9luYdzjaSvTl1yj6yFDA8P09PTQ0dHBz09PQwPD891SGZmbSdLwvkAcF66/BVwN8k0AgvC8PAwfX19jI2NERGMjY3R19fnpGNmgL+QTse0xlLbW0m6PSJ+O4d4cjGbsdR6enoYG9v3imKxWGR0dHSWkZlZOyt9Id29e/feskKhwODgIL29vXMYWWPMxVhqB5ctSyW9HviVRgXQ6rZt2zatcvA3HrOFor+/f1KyAdi9ezf9/f1zFFFry3JJbTPJJbTNwDeBPwfOyjOoVtLd3V2zfGpyOfvss30JzmyBmMkX0oVsRpfU2s1sLqnVajID+3wmiUrn1JfgzOaf+X7JfS4uqT1X0p9JukHSP0k6V9JzGxVAq+vt7WVwcJBisYgkisXi3uuzlZrT1RJ46RuPL7eZzR8DAwMUCoVJZYVCgYGBgTmKqMVFRM0FuB74JHBSugwCn6lXr5WWFStWRB4kBcnspXWXYrEYQ0NDUSgUJpUXCoUYGhrKJT4zy9/Q0FAUi8WQtPf3fL4ARqKBf4vrXlKTdE9EnFCvrJXlNeNnteb0VKVLcP39/VW3LxaLDAwMzIueLWY2PzT9khrwXUknlgXwG8CdjQqgnVVqTley3377AbVvJLpzgZnNd1laOA8ALwZKfy27gQeACSCibOqCVpVXCweSezJr1qxhfHy85nZLlizhgAMO4LHHHqu53Xy52Whm7a/RLZwsCadY6/Nog3HW8kw4AB0dHVU7C5R73vOex8TExD4dDcpJYmJiopHhmZnNyFxcUvubiBgrX8rLagS6QdJOSVvLyi6U9LCku9NldVq+WNImSfdKekDSBVX2WbH+XKv2rM5UTz311N4eb7Pdl5lZu8mScI4rfyNpEbAiQ71rgFMrlF8eEcvT5ea07M3AcyLipem+3yWpp8p+K9WfU1nv5UDSzXp0dJShoSF3pzSzBaVqwpF0gaQngeMlPSHpyfT9I8Dn6+04Iu4AHs8YRwD7p8lsP+CXJPPutIWpz+p0dFQ+rV1dXVXrlD/fY2Y2H1VNOBHx0Yg4ALgkIg6MiAPSpSsiKl7yyui9krakl9wOSss+C/wc2EHSOeHSiKiWrCrV34ekPkkjkkZ27do1i3CzKbVcJiYm+Id/+AcWL1486fPFixezbt26qnVGR0edbMxsXstySe1fJP3W1GWGx7sCOApYTpJcLkvLXwGMAy8EjgT+XNKLplF/HxExGBErI2LlsmXLZhjuzPT29rJx48ZJrZeNGzc6oZjZglZriumS88rWn0uSHDYDJ0/3YBHxSGld0lXATenbPwG+FBHPADsl3QmsBH6YsX7L6e3tdYIxMytTt4UTEb9XtpwCvITkPs60STqs7O3pQKkH2zbgZCX2B04EvjeN+mZm1uKytHCm2k6SdGqSdB2wClgqaTvwQWCVpOUknQRGgXelm/89sJEkgQjYGBFb0v1cDVwZESPAxVXqm5lZi6ubcCT9HckfeEhaRMuBe+rVi4g/rlD8ySrbPkXSNbrSZ/+rbP1t9Y5rZmatKUsLp/wR/T3AdRHhsdTMzGxasiScTwP/jaSV84OI+EW+IZmZ2XxU68HPRZIuJrlnswkYAn4s6WJJi6vVMzMzq6RWL7VLgIOBIyNiRUT8OskzMC8ALm1CbGZmNo/USjhvAN4ZEU+WCiLiCWAt0BKDZpqZWfuolXBKU4xOLRzn2V5rZmZmmdRKOPdLevvUQklvpcJDmWZmZrXU6qX2HuAGSWeSDGUTwMtJRnM+vQmxmZnZPFI14UTEw8BvSDqZZE4cAf8SEbc2KzgzM5s/6j6HExFfBb7ahFjMzGweyzI9gZmZ2aw54ZiZWVM44ZiZWVM44ZiZWVM44ZiZWVM44ZiZWVM44ZiZWVM44ZiZWVM44ZiZWVM44ZiZWVM44ZiZWVM44ZiZWVPklnAkbZC0U9LWsrILJT0s6e50WZ2WL5a0SdK9kh6QdEGVfR4s6cuSHkpfD8orfjMza6w8WzjXAKdWKL88Ipany81p2ZuB50TES4EVwLsk9VSoez5wa0QcDdyavjczszaQW8KJiDuAx7NuDuwvaRHJBG+/BJ6osN1pwKZ0fRPwplmGaWZmTTIX93DeK2lLesmtdEnss8DPgR3ANuDSiKiUrA6NiB0A6esh1Q4iqU/SiKSRXbt2NfifYGZm09XshHMFcBSwnCS5XJaWvwIYB14IHAn8uaQXzeZAETEYESsjYuWyZctmsyszM2uApiaciHgkIsYjYgK4iiTRAPwJ8KWIeCYidgJ3Aisr7OIRSYcBpK87mxG3mZnNXlMTTilZpE4HSj3YtgEnK7E/cCLwvQq7uBFYk66vAT6fV6xmZtZYeXaLvg74JvBiSdslnQVcnHZ93gKcBJybbv73wPNIEtC3gY0RsSXdz9WSSq2di4BTJD0EnJK+NzOzNqCImOsYcrdy5coYGRmZ6zDMzNqKpM0RUen2xox4pAEzM2sKJxwzM2sKJxwzM2sKJxwzM2sKJxwzM2sKJxwzM2sKJ5w6hoeH6enpoaOjg56eHoaHh+c6JDOztrRorgNoZcPDw/T19bF7924AxsbG6OvrA6C3t3cuQzMzaztu4dTQ39+/N9mU7N69m/7+fiB766fR25mZtaWImPfLihUrYiYkBclcPZMWSTE0NBSFQmFSeaFQiKGhoUn7aPR2ZmbNAoxEA/8We2ibGnp6ehgbG9unvFgsAlT9bHR0dO/7pUuX8thjj9XdrtaxyrczM2sWD23TRAMDAxQKhUllhUKBgYEBtm3bVrFOefnw8HDFZDN1u0rv65WbmbUbJ5waent7GRwcpFgsIoliscjg4CC9vb10d3dXrFNeXrrXU2+7Su/rlZuZtRsnnDp6e3sZHR1lYmKC0dHRvb3TarV+Smq1Tsq3y7o/M7N25oQzQ7VaPyXVWiddXV37dKvOsj8zs3bmTgM5mvocDyStFicSM2sH7jTQRtxqMTN7lls4ZmZWkVs4ZmbWlpxwzMysKZxwzMysKZxwzMysKZxwzMysKRZELzVJu4B9R8ZsnqXAo3N4/KzaJU5wrHlolzjBseahUpzFiFjWqAMsiIQz1ySNNLJrYV7aJU5wrHlolzjBseahGXH6kpqZmTWFE46ZmTWFE05zDM51ABm1S5zgWPPQLnGCY81D7nH6Ho6ZmTWFWzhmZtYUTjhmZtYUTjh1SNogaaekrWVlF0p6WNLd6bI6LV8iaaOkeyXdI2lVlX2eIOmb6XZfkHRg2WcXSPq+pAclvb5VY5XUI+m/yvZ75WxjTcvfl/7b75N0cVl53fMi6WBJX5b0UPp60HTqz3WcLXpO35zWm5C0cspnDf1ZzSvW2ZzXnOK8RNL3JG2R9M+SXjCd+q0Q64zPaUR4qbEAvwW8DNhaVnYh8IEK274H2JiuHwJsBjoqbPdt4LfT9TOBj6TrxwL3AM8BjgR+AHS2aKw95cdp0Hk9CfgK8JxSXNM5L8DFwPnp+vnAx2Z7XpscZyue018DXgzcBqwsK8/jZzWvWGd8XnOK83XAonT9Y434OZ2DWGd0Tt3CqSMi7gAez7j5scCtab2dwE+BSg9SvRi4I13/MvAH6fppwKci4umI+BHwfeAVLRrrrFSJdS1wUUQ8XRYXZD8vpwGb0vVNwJumWX+u45yVPGKNiAci4sEKh8vjZzWvWGcspzj/NSL2pG//HTh8OvVbJNYZccKZufemzcwNZZdu7gFOk7RI0pHACuCICnW3Am9M199cts2vAj8u2257WtaKsQIcKem7km6X9JoGxHkM8BpJ30r3+fK0POt5OTQidgCkr4dMs/5cxwmtd06ryeNnNa9YobHntZFxngn8yyzqz1WsMINz6oQzM1cARwHLgR3AZWn5BpL/uBHg48A3gD37VudM4D2SNgMHAL9My1Vh29n2W88r1h1Ad0T8OvBnwD+q7F7UDC0CDgJOBM4DrpckZn9eGn1e84pzIZ9TaJ/z2pA4JfWT/M4Nl4qmUz+jvGKd0TldNL3YDSAiHimtS7oKuCkt3wOcW/bZN4CHKtT/Hsm1USQdA/yP9KPtTG5BHA78pBVjTZvopWb6Zkk/IPk2NZu5vLcDN0RykfguSRMkAwpmPS+PSDosInZIOgwoXT5o9HnNJc4WPae19tvQn9W8Ys3hvM46TklrgDcAr033U9pvy53TSrHO9Jy6hTMD6R+JktNJLjshqSBp/3T9FGBPRNxfof4h6WsH8L+BUg+PG4G3SHpOepnraOCuVoxV0jJJnen6i9JYfzibWIHPASen+zwGWEIyem3W83IjsCZdXwN8vqy8kec1lzhb9JxW0/Cf1bxizeG8zipOSacCfwG8MSJ2l33Ucue0WqwzPqfT7WWw0BbgOpLm4zMk3wrOAq4F7gW2pP9xh8WzPTceBB4g6RlSLNvP1aQ9Z4BzgP9Il4tIR3xIP+sn6THyIPC7rRorSeeB+0juBX0H+L0GxLoEGCJJit8BTq53XqbE2kXSEeKh9PXg2Z7XZsbZouf09HRfTwOPALfk+LOaS6yzOa85xfl9kvsnd6fLlS18TivGOtNz6qFtzMysKXxJzczMmsIJx8zMmsIJx8zMmsIJx8zMmsIJx8zMmsIJx5pC0uGSPq9khOQfSFonaUmGen85y+OukvSqada5RMnIupfM5tgZjjMq6d+mlN2tKaP9zvIYf1m23tPIfdc45jWSzqizzTskvTDvWKy1OOFY7tKhNG4APhcRR5M8kfw8YCBD9VklHGAVMK2EA7wLeFlEnFdeKCmPkTkOkHREuv9fm27l0sN3Ncz2/OXlHYATzgLjhGPNcDLwi4jYCBAR4yTD6pyZjnjwDkmfKG0s6aa0ZXIRsF/6rX84/Yb+PUmblAxG+llJhbTOqKSl6fpKSbdJ6gHeDZyb7uM1SuZM2apkDqA7psSJpBuB/YFvSfqj9Nv630r6GvAxScsl/buenR+kNJfNbZIul3SHpAckvVzSDWmL7m9qnJvrgT9K1/+Y5OG9Uiw9kv5N0nfS5VVp+SpJX5P0jyQP9SLpc5I2py2zvrRs0vlLd9sp6ap0u3+VtF+6ba1/18ck3SXpP1RhkEYlPiHpfklfpGwwUkl/Lenb6TkfTLc9g2Rk8uE0tv0qbVfjnFm7ms6TrF68zGQB3g9cXqH8u8DxJN92P1FWfhOwKl1/qqy8h2SAwd9M328gnesHGAWWpusrgdvS9Qspmw+I5A/0r6brL6gSb/kxr0nj6Uzfb+HZ+YE+DHw8Xb+NZ+cKOYdkXKrDSOYb2Q50VTjOKElr7xtl5+NY0nlGgALw3HT9aGAkXV8F/Bw4smxfpdEK9iN5qryryvnbAyxP318PvDXDv+uydH018JUK/47fJ5m6opOk1fJT4IzyuNL1a0mfSGffOWsqbudlfi1u4VgziMoj0VYrr+XHEXFnuj4EvHqa9e8ErpH0TpI/kFl8JiLGJT2fJEndnpZvIpn0quTG9PVe4L6I2BHJIIc/pPLUD5DMX/Kfkt5CMsxQ+dhai4GrJN0LfIYkGZXcFck8JiXvl3QPyZwlR5AkqEp+FBF3p+ubgZ4M/64byrevsM/fAq6LiPGI+Anw1bLPTlIyNP69JC3d46rElXU7a2NOONYM9zFlcjclQ5kfQTKW0x4m/yw+t8a+piao0vvyfVStHxHvJhmE9Ajgbkld9YInaU1k8XT6OlG2Xnpf6/7Pp4G/p+xyWupckjHBTiA5f+WdLPbGpGR68N8BXhkRJ5C0lKqdg/K4xuvENbVOre33+eIg6bnAepLWzkuBqyrFlXU7a39OONYMtwIFSW+HvTe6LwOuiWQE2lFguaSO9AZ6+cyDz0haXPa+W9Ir0/U/Br6ero+STCIHk2clfZJkHh/SYx8VEd+KiL8mGTW3WstjHxHxM5LWSOk+xtuA22tUyeqfSaadvmVK+fOBHRExkR6rWovs+cB/RsRuSf+dZO6Tkqnnbx8N+HfdQTLycKeS0clPSstLSeNRSc8Dynuulf+/1NrO5hEnHMtdRATJSL5vlvQQycjTv+DZHlR3Aj8iuRR1KcnosyWDwJaym94PAGskbQEOJplgDuBDwDol3YzHy+p/ATi91GkAuETSvUq6B99BMtrtdKxJ97GFZFK7D0+z/j4i4smI+FhE/HLKR+tJ/q3/TnKvp1pL60vAojSmj5BcViuZev6qmc2/659JRr6+l+T/43aAiPgpSWvlXpJh8r9dVuca4EpJd5O0oKptZ/OIR4u2tqGk19lNEfGSuY7FzKbPLRwzM2sKt3DMzKwp3MIxM7OmcMIxM7OmcMIxM7OmcMIxM7OmcMIxM7Om+P85PGZLuAvZKQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = []\n",
    "lstoutput = []\n",
    "lstytest = []\n",
    "for ep in range(1, epochs+1):\n",
    "    train(ep)\n",
    "    Y_test, output, test_loss = evaluate()  \n",
    "    loss.append(test_loss.item())\n",
    "    lstoutput.append(output.tolist())\n",
    "    lstytest.append(Y_test.tolist())\n",
    "    \n",
    "minloss = min(loss)\n",
    "minidx = loss.index(minloss)\n",
    "\n",
    "Y_test = lstytest[minidx]\n",
    "output = lstoutput[minidx]\n",
    "    \n",
    "f, ax = plt.subplots()\n",
    "print(\"min loss found in ep \", minidx, \"\\n\")\n",
    "ax.scatter(Y_test, output,  color='black')\n",
    "ax.set_xlabel('Outputs from Marathon data')\n",
    "ax.set_ylabel('Outputs from predictions')\n",
    "figname = \"tcn_seq_110421_lowest_loss.png\"\n",
    "ax.figure.savefig(figname, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99933488",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb63e52c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
